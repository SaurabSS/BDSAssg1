# Learning Distributed/Parallel Big Data Computing with Hadoop MapReduce


Note: This README is a brief overview of the directory and the steps to get started. For the Analytical Report of experiments, refer to ANALYSIS.md


## Directories Explained

The Input is a collection of all text files that were fed into HDFS during the experiment. It has 2 sets with size of 20 files (small-medium) and a few more large files. If attempting to reproduce the experiment on a subset of files, you must copy the amount of files needed to HDFS using the commands below (see: `Load Data`).

The Output is the collection of outputs generated by WordCount and TopN programs. The naming tries to be indicative of what the output is for.

Screenshots contain snaps of executed programs on a certain amount of data and in certain settings. The README will refer to them, hence no exploration is necessary there. 

WordCount.java is the vanilla Java program in Hadoop MapReduce tutorial `https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html`. It counts the number of occurences of ALL words in any given set of files in an input directory. 

TopN.java is a code that uses MapReduce paradigm to arrive at the Top N most occurred words in a given set of text files. 


## To Run

**Note:**

These instructions assume that Hadoop is installed and is working on your computer(s). If not yet installed, please refer to their instructions to install [here](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html).

Once you installed Hadoop, it is recommended to format your namenode if you are trying to start afresh w.r.t HDFS. 

**Warning**: Doing so will erase all your previous files in HDFS.

Follow these steps to reset your namenode.
```
stop-all.sh
hadoop namenode -format
start-all.sh
```

### Set Environment Variables

```
export JAVA_HOME=/usr/java/default
export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
```

Please ensure your JAVA_HOME location matches with the Input. For reference, my Location is `/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home`.

### Compile
First, compile the mapreduce code. 

```hadoop com.sun.tools.javac.Main foo.java```

where foo = TopN or WordCount
> Run the `hadoop` command as per its location on your system

`$ jar cf wc.jar WordCount*.class` 

`$ jar cf topn.jar TopN*.class`
 
### Load Data
`hadoop fs -mkdir -p /user/input`


`hadoop fs -put /<fs path of file> /user/input`

### Run

`$ hadoop jar wc.jar WordCount /user/input /user/output`

or

`$ hadoop jar topN.jar TopN /user/input /user/output`

### Get the Output back to the File System

`hadoop fs -get /user/output/part-r-<reducernumber> Output`

where the `reducernumber` starts from 00000 

## Software/Tools Used
http://www.randomtextgenerator.com to generate 20 smaller text files . See `Input/text(1?)[0-9].txt`

https://www.gutenberg.org/browse/authors/t#a53

20 books (or their parts), including Huckleberry Finn and Following the Equator, were used from the Gutenberg project as the corpus to run the topN program.

https://corpus.canterbury.ac.nz/descriptions/#artificl

Some very large files(in Calgary, Cantrbry folders) used for scaling tests were picked up from here.